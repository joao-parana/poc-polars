# Introdução

## O Polars e os outros _players_ na área de DataFrames

### **Descrição Completa do Polars vs. Pandas vs. PySpark**

#### **Visão Geral**

1. **Polars**: Biblioteca Python escrita em Rust, otimizada para desempenho em grandes volumes de dados. Combina execução **multithreaded**, **vetorizada** e **avaliação preguiçosa** (lazy execution) para processar dados de forma eficiente. Funciona em um único nó, sem a complexidade de clusters, e integra-se bem com formatos como Parquet.
2. **Pandas**: Biblioteca clássica para manipulação de dados em Python, mas limitada a processamento em memória e execução **eager** (imediata). Ideal para datasets pequenos/médios.
3. **PySpark**: Interface Python do Apache Spark, projetada para processamento **distribuído** em clusters. Oferece avaliação preguiçosa e escalabilidade horizontal, mas exige infraestrutura complexa.

---

### **Tabela Comparativa Geral**

| Característica             | Polars                             | Pandas                            | PySpark                         |
| -------------------------- | ---------------------------------- | --------------------------------- | ------------------------------- |
| **Linguagem Base**         | Rust (via pyO3)                    | Python/C                          | Scala/Java (JVM)                |
| **Execução**               | Eager e Lazy                       | Apenas Eager                      | Lazy (DAG-based)                |
| **Multithreading**         | Nativo (Rust)                      | Limitado (single-threaded)        | Sim (via tasks distribuídas)    |
| **Escalabilidade**         | Alto (single-node)                 | Baixo (memória local)             | Alto (clusters distribuídos)    |
| **Overhead de Setup**      | Zero (biblioteca leve)             | Zero                              | Alto (cluster Spark, JVM, etc.) |
| **Uso de Memória**         | Otimizado (Apache Arrow)           | Alto (objetos Python)             | Gerenciado por Spark (off-heap) |
| **Integração com Parquet** | Excelente (leitura/escrita rápida) | Boa (via `pyarrow`)               | Excelente (formato nativo)      |
| **API**                    | Semelhante a Pandas, mas imutável  | Madura e intuitiva, porém mutável | Verbosa (paradigma funcional)   |

---

### **Pontos Fortes e Fracos**

#### **Polars**

| **Pontos Fortes**                                 | **Pontos Fracos**                       |
| ------------------------------------------------- | --------------------------------------- |
| Velocidade superior (multithreaded + Rust)        | Comunidade menor que Pandas ou PySpark  |
| Execução preguiçosa com otimizações de query      | API imutável (exige reatribuição)       |
| Baixo consumo de memória (formato Arrow)          | Documentação menos detalhada            |
| Suporte nativo a Parquet com _predicate pushdown_ | Menos integração com bibliotecas Python |

#### **Pandas**

| **Pontos Fortes**                                 | **Pontos Fracos**                        |
| ------------------------------------------------- | ---------------------------------------- |
| API intuitiva e consolidada                       | Ineficiente para grandes datasets        |
| Ecossistema rico (Matplotlib, Scikit-learn, etc.) | Sem suporte a lazy execution             |
| Curva de aprendizado baixa                        | Gerenciamento de memória ineficiente     |
| Usa Apache Arrow para alocação zero-copy          | Limitado a single-node (não distribuído) |

#### **PySpark**

| **Pontos Fortes**                                         | **Pontos Fracos**                         |
| --------------------------------------------------------- | ----------------------------------------- |
| Escalabilidade horizontal em clusters                     | Configuração complexa (Spark, JVM, etc.)  |
| Processamento distribuído (TB+ de dados)                  | Overhead de latência em queries simples   |
| Integração com Hadoop/Data Lakes e todo ecosistema Apache | API menos intuitiva (paradigma funcional) |

---

### **Benefícios do Polars em Ambientes com Grandes Volumes de Dados (Parquet/Data Lakes)**

1. **Desempenho Superior em Single-Node**:

   - Processa GBs de dados em segundos, graças ao motor multithreaded e vetorizado em Rust.
   - Exemplo: Agregações em Parquet são **~5–10x mais rápidas** que Pandas.

2. **Otimizações para Parquet**:

   - **Predicate Pushdown**: Filtra dados diretamente no nível de leitura do Parquet.
   - **Projection Pruning**: Ignora colunas irrelevantes durante a leitura.
   - **Leitura Paralela**: Divide arquivos em chunks para processamento simultâneo.

3. **Eficiência de Memória**:

   - Usa Apache Arrow para alocação zero-copy, reduzindo o uso de RAM em até 50% vs. Pandas.

4. **Lazy Execution**:

   - Otimiza o dataflow automaticamente (ex.: fusão de operações, eliminação de colunas).

5. **Sem Dependências Externas**:

   - Não requer clusters, JVMs, ou configurações complexas (ao contrário do PySpark).

6. **Concorrência com PySpark em Casos de Uso**:
   - Em ambientes single-node, substitui PySpark para datasets de até **alguns TB** com melhor custo-benefício.

---

### **Exemplo de Caso de Uso com Parquet**

```python
# Polars (lazy mode)

# Dataset de exemplo:
# NYC Taxi and Limousine Commission (TLC)
# https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
# Baixe os meses/anos que desejar.
# Tamanho dos arquivos: ~ 50GB/mês (Ex.: yellow_tripdata_2024-01.parquet)
# Numa estimativa grosseira, os 16 anos de dados completos podem ocupar ~ 9.6 GB em Parquet.

import time
import pandas as pd
import polars as pl

# 2 meses de dados em 2024 (janeiro e fevereiro) são 100 MB e suficientes para testes iniciais
# valid columns: ["VendorID", "tpep_pickup_datetime", "tpep_dropoff_datetime", "passenger_count",
#   "trip_distance", "RatecodeID", "store_and_fwd_flag", "PULocationID", "DOLocationID", "payment_type",
#   "fare_amount", "extra", "mta_tax", "tip_amount", "tolls_amount", "improvement_surcharge",
#   "total_amount", "congestion_surcharge", "Airport_fee"]

start = time.perf_counter()
df = pl.scan_parquet("data/parquet/nyctlc/*.parquet")
result = (
    df.filter(pl.col("RatecodeID").is_in([2, 3]))
      .group_by(["RatecodeID", "PULocationID"])
      .agg(pl.col("fare_amount").sum())
      .collect()  # Executa a query otimizada em Parquet com 'Predicate Pushdown'' e 'Projection Pruning''
)
end = time.perf_counter()
print(f" Polars: {(end-start)/10:.4f} segundos. Resultado = \n{result}")

start = time.perf_counter()
# Pandas (quase equivalente e ineficiente)
df = pd.read_parquet("data/parquet/nyctlc/*.parquet")
result = df[df["RatecodeID"] == 2].groupby("PULocationID")["fare_amount"].sum()
end = time.perf_counter()
print(f" Pandas: {(end-start)/10:.4f} segundos. Resultado = \n{result}")

print(f"Compare os resultados do shape dos DataFrames no Polars e no Pandas. "
      f"Veja que o Polars tem uma abordagem mais regular em termos de dados Tabulares.")
```

---

### **Conclusão**

O Polars é a escolha ideal para:

- **Processamento rápido** de dados em ambientes single-node.
- **Carga de dados massiva** em Parquet/Data Lakes sem overhead de clusters.
- **Migração de pipelines Pandas** que esbarram em limites de memória/performance.

Enquanto PySpark ainda domina em cenários **petabytes/distribuídos**, o Polars preenche a lacuna entre
Pandas e Spark, oferecendo velocidade e simplicidade para a maioria dos casos modernos de Data Lakes.
